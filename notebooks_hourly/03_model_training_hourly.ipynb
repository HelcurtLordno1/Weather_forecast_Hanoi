{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for multi-horizon model training\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "\n",
    "# Deep Learning\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "    HAS_TENSORFLOW = True\n",
    "except ImportError:\n",
    "    HAS_TENSORFLOW = False\n",
    "    print(\"‚ö†Ô∏è TensorFlow not available - will skip neural network models\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add utilities\n",
    "sys.path.append('../src/hourly')\n",
    "sys.path.append('../src/shared')\n",
    "\n",
    "# File paths\n",
    "FEATURES_PATH = '../data/processed/hanoi_weather_hourly_features.csv'\n",
    "METADATA_PATH = '../data/processed/hourly_feature_metadata.json'\n",
    "MODELS_OUTPUT_PATH = '../models/hourly_trained/'\n",
    "\n",
    "print(\"ü§ñ MULTI-HORIZON WEATHER FORECASTING\")\n",
    "print(\"=\" * 40)\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìÇ Features: {FEATURES_PATH}\")\n",
    "print(f\"üìã Metadata: {METADATA_PATH}\")\n",
    "print(f\"üíæ Models output: {MODELS_OUTPUT_PATH}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(MODELS_OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ad6a3",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Load Features & Setup Multi-Horizon Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81451897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature-engineered data and setup multi-horizon framework\n",
    "print(\"üìä LOADING FEATURE DATA & MULTI-HORIZON SETUP\")\n",
    "print(\"=\" * 47)\n",
    "\n",
    "# Load features\n",
    "df_features = pd.read_csv(FEATURES_PATH)\n",
    "df_features['datetime_processed'] = pd.to_datetime(df_features['datetime_processed'])\n",
    "df_features.set_index('datetime_processed', inplace=True)\n",
    "\n",
    "print(f\"‚úÖ Features loaded: {df_features.shape}\")\n",
    "\n",
    "# Load metadata\n",
    "with open(METADATA_PATH, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"üìã Metadata loaded: {len(metadata)} keys\")\n",
    "\n",
    "# Define forecasting horizons\n",
    "FORECAST_HORIZONS = {\n",
    "    '1h': 1,      # Immediate forecasting\n",
    "    '6h': 6,      # Short-term planning  \n",
    "    '24h': 24,    # Daily planning\n",
    "    '72h': 72,    # 3-day forecast\n",
    "    '168h': 168   # Weekly forecast\n",
    "}\n",
    "\n",
    "# Define target variables for multi-variate forecasting\n",
    "TARGET_VARIABLES = ['temp', 'humidity', 'sealevelpressure', 'windspeed', 'cloudcover']\n",
    "\n",
    "print(f\"üéØ Forecast horizons: {list(FORECAST_HORIZONS.keys())}\")\n",
    "print(f\"üìä Target variables: {TARGET_VARIABLES}\")\n",
    "\n",
    "# Feature selection - remove highly correlated and low-importance features\n",
    "print(f\"\\nüîß FEATURE PREPROCESSING\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Remove non-predictive columns\n",
    "exclude_cols = ['name', 'address', 'resolvedAddress', 'latitude', 'longitude', \n",
    "               'preciptype', 'conditions', 'icon', 'source', 'date']\n",
    "\n",
    "# Get numeric features only\n",
    "numeric_cols = df_features.select_dtypes(include=[np.number]).columns\n",
    "feature_cols = [col for col in numeric_cols if col not in exclude_cols + TARGET_VARIABLES]\n",
    "\n",
    "print(f\"Available numeric features: {len(numeric_cols)}\")\n",
    "print(f\"Selected feature columns: {len(feature_cols)}\")\n",
    "\n",
    "# Handle remaining missing values\n",
    "missing_count = df_features[feature_cols].isnull().sum().sum()\n",
    "if missing_count > 0:\n",
    "    print(f\"‚ö†Ô∏è Handling {missing_count} missing values...\")\n",
    "    df_features[feature_cols] = df_features[feature_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    print(\"‚úÖ Missing values filled\")\n",
    "\n",
    "print(f\"‚úÖ Feature preprocessing completed!\")\n",
    "print(f\"üìä Final feature set: {len(feature_cols)} features\")\n",
    "print(f\"üìÖ Date range: {df_features.index.min()} to {df_features.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0fd23",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Multi-Horizon Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe68f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets for different forecasting horizons\n",
    "print(\"‚è∞ MULTI-HORIZON DATA PREPARATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "def create_multi_horizon_datasets(df, target_var, feature_cols, horizons, test_size=0.2):\n",
    "    \"\"\"Create datasets for multiple forecasting horizons\"\"\"\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    for horizon_name, horizon_hours in horizons.items():\n",
    "        print(f\"üìä Preparing {horizon_name} dataset (target: {target_var})...\")\n",
    "        \n",
    "        # Create target variable shifted by horizon\n",
    "        df_horizon = df.copy()\n",
    "        df_horizon[f'{target_var}_target'] = df_horizon[target_var].shift(-horizon_hours)\n",
    "        \n",
    "        # Remove rows where target is NaN (at the end due to shift)\n",
    "        df_horizon = df_horizon.dropna(subset=[f'{target_var}_target'])\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = df_horizon[feature_cols].values\n",
    "        y = df_horizon[f'{target_var}_target'].values\n",
    "        \n",
    "        # Time series split (preserve temporal order)\n",
    "        split_idx = int(len(X) * (1 - test_size))\n",
    "        \n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        # Store metadata about the split\n",
    "        train_dates = df_horizon.index[:split_idx]\n",
    "        test_dates = df_horizon.index[split_idx:]\n",
    "        \n",
    "        datasets[horizon_name] = {\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test,\n",
    "            'train_dates': train_dates,\n",
    "            'test_dates': test_dates,\n",
    "            'feature_names': feature_cols,\n",
    "            'target_name': f'{target_var}_target',\n",
    "            'horizon_hours': horizon_hours\n",
    "        }\n",
    "        \n",
    "        print(f\"   Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "        print(f\"   Train period: {train_dates.min()} to {train_dates.max()}\")\n",
    "        print(f\"   Test period: {test_dates.min()} to {test_dates.max()}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Create datasets for each target variable\n",
    "print(\"üéØ Creating multi-horizon datasets for all targets...\")\n",
    "\n",
    "all_datasets = {}\n",
    "for target in TARGET_VARIABLES:\n",
    "    print(f\"\\nüå°Ô∏è Processing target: {target}\")\n",
    "    target_datasets = create_multi_horizon_datasets(\n",
    "        df_features, target, feature_cols, FORECAST_HORIZONS\n",
    "    )\n",
    "    all_datasets[target] = target_datasets\n",
    "\n",
    "print(f\"\\n‚úÖ Multi-horizon datasets created!\")\n",
    "print(f\"üìä Total datasets: {len(TARGET_VARIABLES)} targets √ó {len(FORECAST_HORIZONS)} horizons = {len(TARGET_VARIABLES) * len(FORECAST_HORIZONS)}\")\n",
    "\n",
    "# Feature scaling preparation\n",
    "print(f\"\\n‚öñÔ∏è FEATURE SCALING SETUP\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "scalers = {}\n",
    "for target in TARGET_VARIABLES:\n",
    "    scalers[target] = {}\n",
    "    for horizon in FORECAST_HORIZONS.keys():\n",
    "        # Use RobustScaler for weather data (handles outliers better)\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        # Fit on training data\n",
    "        X_train = all_datasets[target][horizon]['X_train']\n",
    "        scaler.fit(X_train)\n",
    "        \n",
    "        # Transform both train and test\n",
    "        all_datasets[target][horizon]['X_train_scaled'] = scaler.transform(X_train)\n",
    "        all_datasets[target][horizon]['X_test_scaled'] = scaler.transform(all_datasets[target][horizon]['X_test'])\n",
    "        \n",
    "        scalers[target][horizon] = scaler\n",
    "\n",
    "print(\"‚úÖ Feature scaling completed for all datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98872d5a",
   "metadata": {},
   "source": [
    "# ü§ñ Hanoi Hourly Weather Model Training - Step 5\n",
    "\n",
    "This notebook implements multi-horizon forecasting models for hourly weather prediction, comparing different algorithms and prediction windows.\n",
    "\n",
    "**Multi-Horizon Strategy:**\n",
    "- **Short-term (1-6h)**: High accuracy for immediate forecasting\n",
    "- **Medium-term (12-24h)**: Daily planning and pattern recognition\n",
    "- **Long-term (48-168h)**: Weekly trends and seasonal patterns\n",
    "- **Model Comparison**: XGBoost, LightGBM, CatBoost, Neural Networks\n",
    "- **Evaluation**: MAE, RMSE, MAPE for different horizons\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
